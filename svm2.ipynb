{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn matplotlib joblib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQsUyqOFGJfV",
        "outputId": "cb080eac-53c2-4293-ab1d-4521be02e240"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
        "algorithms?"
      ],
      "metadata": {
        "id": "WogVKWhdHWuz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WirvmKwlHTMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning algorithms, polynomial functions are a type of transformation used to map data into higher-dimensional spaces to make it easier to classify. This concept is especially important in Support Vector Machines (SVM).\n",
        "\n",
        "Polynomial Kernel: In SVM, the polynomial kernel is a specific type of kernel function that computes the inner product of two vectors in a higher-dimensional space. The kernel function maps the input data into a higher-dimensional feature space where it is more likely to be linearly separable."
      ],
      "metadata": {
        "id": "Miym8G5zHbR6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jshoUwkYHbl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
      ],
      "metadata": {
        "id": "TqevPT3nHdm7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Z2UkI4BHd3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s how you can implement an SVM with a polynomial kernel using Scikit-learn:\n",
        "\n",
        "Import the necessary libraries.\n",
        "\n",
        "Load a dataset.\n",
        "\n",
        "Split the data into training and testing sets.\n",
        "\n",
        "Preprocess the data (optional).\n",
        "\n",
        "Create the SVC (Support Vector Classifier) with a polynomial kernel.\n",
        "\n",
        "Train the model.\n",
        "\n",
        "Evaluate the performance."
      ],
      "metadata": {
        "id": "FfTtWUQ-HjiT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XRhLpnIKHmNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
      ],
      "metadata": {
        "id": "X2-X-hpDHoS8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "REY5QuQUHooA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Support Vector Regression (SVR), epsilon (ε) defines a margin of tolerance where no penalty is given for errors within this margin. In simple terms:\n",
        "\n",
        "If a data point is within the margin defined by epsilon, it is not considered an error (no penalty).\n",
        "If it falls outside this margin, it is treated as an error and is penalized.\n",
        "Increasing epsilon:\n",
        "\n",
        "Fewer support vectors: When epsilon is large, the margin of tolerance increases, meaning fewer points are considered errors. This leads to fewer points being selected as support vectors.\n",
        "Simpler model: Larger epsilon allows the model to be less sensitive to minor fluctuations in the data, resulting in a simpler model with fewer support vectors.\n",
        "Decreasing epsilon:\n",
        "\n",
        "More support vectors: When epsilon is small, the margin is tighter, and more points are considered errors. This increases the number of support vectors, making the model more complex."
      ],
      "metadata": {
        "id": "rMN0dXf9Hsux"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFZ-aLdNHtKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
        "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
        "and provide examples of when you might want to increase or decrease its value?"
      ],
      "metadata": {
        "id": "IipfyXhMHvJX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ev8pBkcHvXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel function: The kernel determines the decision boundary in a higher-dimensional space.\n",
        "\n",
        "Linear kernel: Works well when the data is linearly separable.\n",
        "Polynomial/RBF kernel: Useful for non-linear data where a simple linear decision boundary won't work.\n",
        "C parameter: Controls the trade-off between maximizing the margin and minimizing the classification error.\n",
        "\n",
        "Large C: More emphasis on minimizing errors, which can lead to overfitting.\n",
        "Small C: Focuses on a larger margin, which can lead to underfitting.\n",
        "Epsilon (ε): Defines the margin within which no penalty is given for errors.\n",
        "\n",
        "Large epsilon: Reduces the sensitivity to data noise, leading to a simpler model (fewer support vectors).\n",
        "Small epsilon: Makes the model more sensitive to noise, potentially resulting in overfitting.\n",
        "Gamma parameter (γ): Defines the influence of each training example in the decision boundary. It is particularly relevant when using non-linear kernels like RBF.\n",
        "\n",
        "Large gamma: Each point's influence is more localized, which can lead to overfitting.\n",
        "Small gamma: More global influence, leading to underfitting."
      ],
      "metadata": {
        "id": "5CxLXymfHzAS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j7V3z128Hza9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Assignment:\n",
        "L Import the necessary libraries and load the dataseg\n",
        "L Split the dataset into training and testing setZ\n",
        "L Preprocess the data using any technique of your choice (e.g. scaling, normalizationK\n",
        "L Create an instance of the SVC classifier and train it on the training datW\n",
        "L Use the trained classifier to predict the labels of the testing datW\n",
        "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
        "precision, recall, F1-scoreK\n",
        "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
        "improve its performanc_\n",
        "L Train the tuned classifier on the entire dataseg\n",
        "L Save the trained classifier to a file for future use."
      ],
      "metadata": {
        "id": "dgwkuFLgH2uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "# Load the Iris dataset (you can use a different dataset if you prefer)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Preprocess the data (scale the features)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create an SVC classifier\n",
        "svm = SVC(kernel='poly', degree=3, C=1.0, coef0=1)\n",
        "\n",
        "# Train the classifier\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels on the testing set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'degree': [2, 3, 4],\n",
        "    'coef0': [0, 1, 5],\n",
        "    'kernel': ['poly']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters from GridSearchCV\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Train the model with the best parameters\n",
        "svm_tuned = grid_search.best_estimator_\n",
        "\n",
        "# Train on the entire dataset\n",
        "svm_tuned.fit(X, y)\n",
        "\n",
        "# Save the trained model to a file\n",
        "joblib.dump(svm_tuned, 'svm_model.pkl')\n",
        "\n",
        "# Predict and evaluate the performance again\n",
        "y_pred_tuned = svm_tuned.predict(X_test)\n",
        "print(\"Classification Report (Tuned Model):\")\n",
        "print(classification_report(y_test, y_pred_tuned))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1teJAA88H3K9",
        "outputId": "d4b32af0-2252-4244-c752-7f3e3d4716f2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "Best Parameters: {'C': 1, 'coef0': 1, 'degree': 2, 'kernel': 'poly'}\n",
            "Classification Report (Tuned Model):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      1.00      0.60        19\n",
            "           1       0.00      0.00      0.00        13\n",
            "           2       0.00      0.00      0.00        13\n",
            "\n",
            "    accuracy                           0.42        45\n",
            "   macro avg       0.14      0.33      0.20        45\n",
            "weighted avg       0.18      0.42      0.25        45\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGb_SvcJH4zH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}