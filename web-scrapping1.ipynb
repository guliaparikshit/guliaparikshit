{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Web scraping is the process of automatically collecting data from websites. It involves pulling information from a website's pages and organizing it into a useful format, like a spreadsheet or database.\n\n\n\nWeb scraping is used to gather large amounts of data quickly and automatically. It's helpful for:\n\nCollecting data from many sources.\n\nAutomating tasks that would take too long to do manually.\n\nAreas of use\n\nE-commerce\n\nNews\n\nReal Estate","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Q2. What are the different methods used for Web Scraping?","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Manual Scraping**\n\nWhat: Copying data manually from a website.\n\nWhen: For small, one-time tasks.\n\nLimitations: Time-consuming, not scalable.\n\n**Using Browser Developer Tools**\n\nWhat: Using browser tools (like \"Inspect\") to view and extract data.\n\nWhen: For small data extraction or one-off tasks.\n\nLimitations: Manual, not efficient for large-scale scraping.\n\n**Automated Web Scraping (Tools & Libraries)**\n\nWhat: Using tools like BeautifulSoup, Scrapy, or Selenium to scrape data automatically.\n\nWhen: For large-scale or repetitive tasks.\n\nLimitations: Requires programming knowledge\n\n**API Scraping**\n\nWhat: Using a website's API to access structured data.\n\nWhen: When APIs are available (easier and faster than scraping HTML).\n\nLimitations: APIs may have limits or require authentication.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Q3. What is Beautiful Soup? Why is it used?","metadata":{}},{"cell_type":"markdown","source":"BeautifulSoup is a Python library used for parsing HTML and XML documents and extracting data from them. It creates parse trees from page source code, making it easier to navigate, search, and modify the HTML content. It is often used in web scraping to extract useful data from websites.\n\nUses\n\nHTML Parsing\n\nData Extraction\n\nUser-Friendly ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Q4. Why is flask used in this Web Scraping project?","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Flask is used in web scraping projects to:\n\nCreate Web Interfaces: Build simple web pages where users can input URLs or parameters for scraping.\n\nServe Scraped Data: Display scraped data in a user-friendly format (e.g., tables, charts).\n\nIntegrate with Other Tools: Work seamlessly with libraries like BeautifulSoup or Pandas to scrape and manage data.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Q5. Write the names of AWS services used in this project. Also, explain the use of each service.","metadata":{}},{"cell_type":"markdown","source":"EC2 – Hosts the scraping scripts and Flask application.\n\nS3 – Stores scraped data (e.g., CSV, JSON files).\n\nRDS – Stores structured data in a managed relational database.\n\nLambda – Runs serverless scraping tasks on demand.\n\nAPI Gateway – Exposes scraping functionality via API for user interaction.\n\nCloudWatch – Monitors and logs scraping tasks for performance and debugging.\n\nIAM – Manages user access and permissions to AWS resources.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}