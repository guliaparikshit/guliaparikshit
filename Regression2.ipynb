{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wR8LxS559T_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Concept of R-squared in linear regression.\n",
        "\n",
        "R-squared measures the proportion of variance in the dependent variable explained by the independent variables in a linear regression model.\n",
        "\n",
        "Interpretation: It ranges from 0 to 1; a higher R-squared means a better fit of the model to the data."
      ],
      "metadata": {
        "id": "bL5gOoDL9hGC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PW-FJMfx9hol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Adjusted R-squared vs. R-squared.\n",
        "\n",
        "Adjusted R-squared adjusts the R-squared value for the number of predictors in the model, penalizing the addition of irrelevant predictors.\n",
        "\n",
        "Difference: Unlike R-squared, adjusted R-squared can decrease if additional variables don't improve the model. It’s a better metric when comparing models with different numbers of predictors."
      ],
      "metadata": {
        "id": "yWHSJrPT9kJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7WENCuhQ9lt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. When to use adjusted R-squared.\n",
        "\n",
        "Use adjusted R-squared when comparing models with different numbers of predictors. It provides a more accurate measure of model performance by accounting for the number of variables and avoiding overfitting."
      ],
      "metadata": {
        "id": "UM4vsOzy9nxU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVhZ_X-X9oar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. RMSE, MSE, and MAE in regression analysis.\n",
        "\n",
        "RMSE (Root Mean Squared Error):\n",
        "\n",
        "Measures the average error in the same units as the dependent variable, penalizing large errors more.\n",
        "\n",
        "Average of squared errors, penalizing larger errors more heavily than smaller errors.\n",
        "\n",
        "Measures the average absolute error, treating all errors equally."
      ],
      "metadata": {
        "id": "PTalzhmB9rBl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mgnaopbP9rga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Advantages and disadvantages of RMSE, MSE, and MAE.\n",
        "\n",
        "RMSE: Sensitive to large errors, useful when large errors are particularly undesirable.\n",
        "\n",
        "MSE: Also penalizes large errors heavily, but the squared units make it harder to interpret.\n",
        "\n",
        "MAE: Less sensitive to large errors, easier to interpret but doesn’t highlight large errors.\n",
        "\n",
        "Disadvantage: RMSE and MSE are sensitive to outliers, while MAE is more robust to them."
      ],
      "metadata": {
        "id": "LMQ6rA4Q97Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-GhMcum99E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Lasso vs. Ridge regularization.\n",
        "\n",
        "Lasso (Least Absolute Shrinkage and Selection Operator): Adds a penalty proportional to the absolute value of coefficients, leading to some coefficients being exactly zero (feature selection).\n",
        "\n",
        "Ridge regularization: Adds a penalty proportional to the square of the coefficients, shrinking them towards zero without eliminating any.\n",
        "\n",
        "When to use: Lasso is useful when you suspect that many features are irrelevant (sparse solutions), while Ridge is better when you believe most features are relevant."
      ],
      "metadata": {
        "id": "aHG0qFjP9-5H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1e43kIc-ARc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Regularized linear models and overfitting.\n",
        "\n",
        "Regularized models (like Lasso and Ridge) add a penalty term to the loss function, discouraging overly complex models with large coefficients.\n",
        "\n",
        "Example: In a high-dimensional dataset, Ridge or Lasso regularization can prevent the model from fitting noise in the training data, thus reducing overfitting."
      ],
      "metadata": {
        "id": "wyy45zTY-Cn1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ztG_95F-Dbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Limitations of regularized linear models.\n",
        "\n",
        "Limitations: Regularization methods can be overly simplistic, potentially underfitting the data. They also require careful tuning of the regularization parameter, which can be challenging.\n",
        "\n",
        "Not always best: In cases where complex relationships between variables are important, regularized models may miss crucial patterns."
      ],
      "metadata": {
        "id": "XsKULKe7-FQb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R9F1u-dj-GVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Choosing between RMSE and MAE for model comparison.\n",
        "\n",
        "Model A (RMSE = 10) vs. Model B (MAE = 8):\n",
        "\n",
        "RMSE is more sensitive to large errors, so if you care about penalizing large errors, Model A might be better.\n",
        "\n",
        "MAE treats all errors equally, so Model B might be better if you want to prioritize consistent performance across all predictions.\n",
        "\n",
        "Limitations: RMSE is sensitive to outliers, while MAE provides a more robust evaluation in the presence of outliers."
      ],
      "metadata": {
        "id": "ceCqIh4c-I6M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2r78rqpz-K-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Ridge vs. Lasso regularization model comparison.\n",
        "\n",
        "Ridge (λ = 0.1) vs. Lasso (λ = 0.5):\n",
        "\n",
        "Ridge tends to keep all features but shrinks coefficients, so it’s suitable when all features are important.\n",
        "\n",
        "Lasso can eliminate irrelevant features by setting some coefficients to zero, making it useful when you suspect many features are redundant.\n",
        "\n",
        "Trade-offs: Lasso can lead to sparsity (feature selection), but might miss important variables; Ridge retains all variables but may overfit if the regularization is too weak. The choice depends on the expected relevance of the features and the model's complexity."
      ],
      "metadata": {
        "id": "E_mZerrr-OGo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wxqcd4HP-QdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}