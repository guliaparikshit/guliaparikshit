{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wR8LxS559T_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
        "\n",
        "Ridge Regression is a type of regularized linear regression that adds a penalty term to the loss function based on the sum of the squared coefficients. This helps prevent overfitting by shrinking the coefficients.\n",
        "\n",
        "Difference from OLS (Ordinary Least Squares):\n",
        "\n",
        "OLS minimizes the sum of squared residuals without any regularization, while Ridge Regression minimizes the sum of squared residuals plus a penalty term proportional to the square of the coefficients."
      ],
      "metadata": {
        "id": "73_DsBtT-yJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGAGfmLD-0B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the assumptions of Ridge Regression?\n",
        "\n",
        "Ridge Regression assumes the same basic conditions as ordinary least squares regression:\n",
        "\n",
        "Linearity between the independent and dependent variables.\n",
        "\n",
        "Independence of residuals.\n",
        "\n",
        "Homoscedasticity (constant variance of residuals).\n",
        "\n",
        "Normally distributed errors (optional for OLS but not required for Ridge).\n",
        "\n",
        "Ridge adds the assumption that a penalty term on the coefficients is beneficial for model generalization."
      ],
      "metadata": {
        "id": "N3cgFIRn-4YL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w7tfmGdq-76n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
        "\n",
        "The tuning parameter lambda (λ) controls the strength of regularization. It can be selected using methods like:\n",
        "\n",
        "Cross-validation: Split the data into training and validation sets and choose λ that minimizes the validation error.\n",
        "\n",
        "Grid search: Test a range of λ values and choose the best one based on model performance.\n",
        "\n",
        "L-curve method: Plot the error vs. the magnitude of the coefficients for different λ values and select where the curve bends."
      ],
      "metadata": {
        "id": "6tkM4PhB-_YV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ksHlsZau_BQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
        "\n",
        "No, Ridge Regression is not typically used for feature selection because it shrinks the coefficients but does not set them to zero. Unlike Lasso, which can eliminate features entirely by setting some coefficients to zero, Ridge only reduces the size of the coefficients.\n"
      ],
      "metadata": {
        "id": "Hma-6phW_DUO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rwo9yD3S_D56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
        "\n",
        "Ridge Regression performs well in the presence of multicollinearity because the regularization helps to shrink the coefficients of correlated predictors, reducing the model’s sensitivity to multicollinearity. This prevents the model from assigning inflated weights to correlated variables."
      ],
      "metadata": {
        "id": "2CvWLJAd_GYq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vRQCpfTz_G2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
        "\n",
        "Yes, Ridge Regression can handle both categorical and continuous variables, but categorical variables must be encoded first (e.g., using one-hot encoding or label encoding). Ridge can then use these encoded variables along with continuous ones in the model."
      ],
      "metadata": {
        "id": "3F6_0I1s_I3E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-CJeyXN_LXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How do you interpret the coefficients of Ridge Regression?\n",
        "\n",
        "The coefficients of Ridge Regression are interpreted similarly to those in ordinary least squares regression, but they are typically smaller in magnitude due to regularization. They represent the change in the dependent variable for a one-unit change in the corresponding independent variable, with the effect of multicollinearity reduced."
      ],
      "metadata": {
        "id": "-aOZOZLh_K_j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQVGZuAO_Ngg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
        "\n",
        "Yes, Ridge Regression can be used for time-series data analysis, particularly when dealing with multicollinearity or when trying to regularize models with many predictors (like lagged values, moving averages, etc.). Time-series data may require additional steps like ensuring stationarity and accounting for autocorrelation, but Ridge can be a useful method when there are many correlated features."
      ],
      "metadata": {
        "id": "aood4wUu_QEO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29hn_VwF_QaD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}