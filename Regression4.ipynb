{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wR8LxS559T_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
        "\n",
        "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that adds a penalty term to the loss function based on the sum of the absolute values of the coefficients. This regularization encourages sparsity, meaning it can shrink some coefficients exactly to zero, effectively performing feature selection.\n",
        "\n",
        "Difference from other regression techniques:\n",
        "\n",
        "Unlike OLS (Ordinary Least Squares), which only minimizes the residual sum of squares, Lasso includes a penalty to prevent overfitting.\n",
        "Ridge Regression also regularizes, but it uses the sum of squared coefficients instead of the sum of absolute values, which does not lead to zero coefficients."
      ],
      "metadata": {
        "id": "b16Om8xu_hog"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zxebbxc0_jzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
        "\n",
        "The main advantage is that Lasso Regression can set some coefficients exactly to zero, effectively selecting a subset of the most important features. This makes it useful for models with many predictors, where some variables may be irrelevant or redundant."
      ],
      "metadata": {
        "id": "ov2VSmCO_maz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HPQuzWxT_m0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
        "\n",
        "The coefficients of a Lasso Regression model represent the change in the dependent variable for a one-unit change in the independent variable, with the additional regularization effect.\n",
        "\n",
        "If a coefficient is zero, the corresponding feature has no impact on the prediction.\n",
        "\n",
        "Non-zero coefficients show the influence of the feature on the model’s predictions, with the regularization reducing their magnitudes."
      ],
      "metadata": {
        "id": "RhvFnLJF_oxg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N3LlBcDS_qKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
        "\n",
        "The main tuning parameter in Lasso Regression is lambda (λ), the regularization parameter.\n",
        "\n",
        "Higher λ values result in stronger regularization, which may shrink more coefficients to zero, potentially leading to underfitting.\n",
        "\n",
        "Lower λ values result in weaker regularization, which may allow the model to fit the data too closely and overfit.\n",
        "\n",
        "The optimal λ is typically chosen via cross-validation."
      ],
      "metadata": {
        "id": "OhcgA5kQ_sZg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GwauMBqo_uCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
        "\n",
        "Yes, Lasso Regression can be adapted for non-linear problems by applying non-linear transformations to the input features.\n",
        "\n",
        "For example, you can use polynomial features, interaction terms, or kernel methods (like the kernel trick in support vector machines) before applying Lasso to capture non-linear relationships between variables."
      ],
      "metadata": {
        "id": "rPPxfJq7_wfg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CFxwETNx_x_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
        "\n",
        "Ridge Regression adds a penalty based on the sum of squared coefficients (L2 regularization), which tends to shrink coefficients but does not eliminate them.\n",
        "\n",
        "Lasso Regression adds a penalty based on the sum of absolute values of coefficients (L1 regularization), which can shrink some coefficients exactly to zero, effectively performing feature selection.\n",
        "\n",
        "Key difference: Ridge keeps all features in the model, while Lasso can eliminate irrelevant ones."
      ],
      "metadata": {
        "id": "gRvZxeOC_0YK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXw8VE60_1on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
        "\n",
        "Yes, Lasso Regression can handle multicollinearity by shrinking the coefficients of highly correlated features towards zero.\n",
        "\n",
        "In cases of severe multicollinearity, Lasso may set some coefficients to zero, effectively removing redundant variables from the model and reducing the impact of multicollinearity."
      ],
      "metadata": {
        "id": "LJ-hx4Du_3jm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RzdvGtZG_48p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
        "\n",
        "The optimal value of lambda (λ) can be chosen using methods such as:\n",
        "Cross-validation: Split the data into training and validation sets, and select λ that minimizes the validation error.\n",
        "\n",
        "Grid search: Test a range of λ values and choose the one with the best performance.\n",
        "\n",
        "Information criteria like AIC/BIC may also be used, though cross-validation is most common for Lasso."
      ],
      "metadata": {
        "id": "AlsC1bJi_8Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cr727j8v_-M-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}